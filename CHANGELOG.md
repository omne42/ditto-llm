# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added

- Gateway: add LiteLLM-compatible key management endpoints (`/key/generate`, `/key/update`, `/key/regenerate` (or `/key/:key/regenerate`), `/key/delete`, `/key/info`, `/key/list`) gated by Ditto admin auth.
- Gateway: accept LiteLLM-style OpenAI-compatible endpoints without the `/v1` prefix (e.g. `/chat/completions`, `/embeddings`, `/moderations`, `/files/*`, `/batches/*`, `/models/*`, `/responses/*`).
- Gateway: accept LiteLLM virtual key header `x-litellm-api-key` (optionally `Bearer ...`) across gateway endpoints and strip it before proxying upstream when virtual keys are enabled.
- Gateway: add Anthropic Messages endpoint aliases without the `/v1` prefix (`POST /messages` and `POST /messages/count_tokens`).
- Gateway: add LiteLLM-like A2A agent gateway proxy endpoints (`/a2a/*` and `/v1/a2a/*`) backed by a new `a2a_agents` registry in `gateway.json`/`gateway.yaml`.
- Gateway: add LiteLLM-like MCP gateway endpoints (`/mcp`, `/mcp/tools/list`, `/mcp/tools/call`) backed by a new `mcp_servers` registry in `gateway.json`/`gateway.yaml`.
- Gateway: add MCP tools integration for `POST /v1/chat/completions` via `tools: [{"type":"mcp", ...}]` (multi-server tool name prefixing and optional auto-execution when `require_approval: "never"`).
- Gateway: add MCP tools integration for `POST /v1/responses` via `tools: [{"type":"mcp", ...}]` (tool rewrite and optional auto-execution when `require_approval: "never"`; supports the Responses shim fallback when upstream lacks `/v1/responses`).
- Gateway: allow MCP tool auto-execution to run multiple rounds via `max_steps` (default `1`, max `8`).
- Gateway: cache MCP `tools/list` results (60s TTL) and fetch them concurrently when multiple servers are selected.
- Gateway: add tenant-scoped admin tokens (read/write) to enforce per-tenant isolation in `/admin/*` (new CLI flags: `--admin-tenant-token*` / `--admin-tenant-read-token*`).
- Gateway: add audit export endpoint (`GET /admin/audit/export`) with JSONL/CSV output and a tamper-evident SHA-256 hash-chain (`prev_hash`/`hash`), plus a verifier CLI (`ditto-audit-verify`).
- Gateway: add `observability.redaction` config to apply a unified redaction policy to JSON logs, audit logs, and devtools JSONL output.
- Gateway: add `ditto-audit-export` CLI to fetch `/admin/audit/export`, write a local export + manifest, and optionally upload to S3/GCS (supports S3 Object Lock flags).
- Gateway: add per-route Redis rate limiting (route-scoped keys) and upgrade the algorithm to a weighted sliding window across the current and previous minute (60s window).
- Security: add `secret://...` secret resolution (env/file/Vault/AWS SM/GCP SM/Azure KV) and wire it into SDK `ProviderAuth`, Gateway config values, and gateway CLI flags (admin tokens / redis url).
- SDK: add `CacheLayer` middleware (implements `LanguageModelLayer`) with deterministic request hashing and streaming replay.
- SDK: add Vercel AI SDK UI message stream SSE adapter (`ui_message_stream_v1_sse`).
- SDK: add `UI_MESSAGE_STREAM_V1_HEADERS` constant for Vercel AI SDK UI message stream (v1) responses.
- SDK: add `sdk-axum` feature and an `axum` response helper (`ui_message_stream_v1_sse_response`) for UI message streams.
- Deploy/Observability: add a Helm chart (`deploy/helm/ditto-gateway`), a Grafana dashboard template (`deploy/grafana/ditto-gateway.dashboard.json`), and a PrometheusRule template (`deploy/prometheus/ditto-gateway-prometheusrule.yaml`).
- JS/TS: add a minimal stream protocol v1 client (`packages/ditto-client`) and React hook (`packages/ditto-react`).
- Admin UI: add a minimal React admin console for `ditto-gateway` (`apps/admin-ui`) plus minimal multi-language gateway client examples (Node/Python/Go).
- Docs: add an mdBook-based `docs/` handbook (SDK + Gateway + migration + roadmap).
- Docs: expand SDK/Gateway guides with recipes and advanced topics (agents, middleware, stream protocol v1, devtools/telemetry/MCP).
- Docs: add Gateway MCP documentation and update the LiteLLM/AI SDK parity notes.
- Docs: refresh docs information architecture (new templates page, JS/React client docs, updated homepage/navigation) and add a repo-root `llms.txt` entrypoint.
- Docs: add a roadmap gap-analysis page (vs LiteLLM + AI SDK) and update the parity checklist.
- Docs: align README gateway-translation endpoint list (include `/v1/files*` and `GET /v1/models/*`) and document optional YAML config (`gateway-config-yaml`).
- Docs: refresh the superset roadmap page to reflect shipped P0/P1 slices.
- SDK: add `StreamTextHandle` / `StreamObjectHandle` plus `into_*_stream` helpers to avoid holding unused streaming fan-out receivers.
- Docs: make repo-root `llms.txt` include an auto-generated docs bundle (based on `docs/src/SUMMARY.md`) and add a generator CLI (`ditto-llms-txt`) to refresh it.
- Docs: mirror `llms.txt` into `docs/src/llms.txt` so `mdbook build docs` publishes it at `/llms.txt`, and update `ditto-llms-txt` to refresh both outputs by default.
- Gateway: add proxy cache size caps (`max_body_bytes` and `max_total_body_bytes`) and CLI flags (`--proxy-cache-max-body-bytes`, `--proxy-cache-max-total-body-bytes`).
- Gateway: add optional YAML config support (rebuild with `--features gateway-config-yaml`) without making a YAML parser a default dependency.
- Gateway: when built with `--features gateway-config-yaml`, allow `ditto-gateway` to read LiteLLM `proxy_config.yaml` / `proxy_server_config.yaml` and import them into a Ditto gateway config (via `model_list` + `general_settings.master_key`).
- Gateway: when using Redis store, enforce global rpm/tpm limits via Redis atomic counters (multi-replica consistent).
- Gateway: add optional project/user shared rate limits (`virtual_keys[].project_limits` / `virtual_keys[].user_limits`) for enterprise quotas (works with in-memory and Redis modes).
- Gateway: add optional tenant attribution + shared quotas (`virtual_keys[].tenant_id` + `tenant_budget` / `tenant_limits`) and admin aggregated ledger views (`/admin/budgets/tenants` / `/admin/costs/tenants`).
- Gateway: add Admin API filters for large deployments (`GET /admin/keys` filters and `key_prefix` for `/admin/budgets` and `/admin/costs`).
- Gateway: add Admin API pagination (`limit`/`offset`) for `GET /admin/keys`, `GET /admin/budgets`, and `GET /admin/costs` (caps `limit` at 10000).
- Gateway: add a maintenance endpoint to reap stale persistent budget reservations (`POST /admin/reservations/reap`) to recover from crashed/aborted requests.
- Gateway: add RBAC-lite admin auth split: read-only admin token (`--admin-read-token` / `--admin-read-token-env`) for read endpoints, plus write admin token (`--admin-token` / `--admin-token-env`) for mutations.
- Gateway: add `--audit-retention-secs` to prune audit logs for sqlite/redis stores.
- Gateway: default audit retention to 30 days when sqlite/redis store is enabled (set `--audit-retention-secs 0` to disable pruning).
- Gateway: add `--proxy-max-body-bytes` to cap `/v1/*` request body size (memory safety / DoS hardening).
- Gateway: add `--proxy-usage-max-body-bytes` to cap response buffering for `usage` parsing (decoupled from proxy cache caps).
- Gateway: add control-plane cache size caps (`cache.max_body_bytes` and `cache.max_total_body_bytes`) for `/v1/gateway`.
- Deploy: add `Dockerfile`, `deploy/docker-compose.yml`, and `deploy/k8s/*` templates for `ditto-gateway`.
- Docs: add runnable Docker Compose + Kubernetes deployment template pages.
- Utils: add bounded SSE parsing with `SseLimits` (max line/event bytes) to reduce OOM risk.

### Changed

- Deps: update Rust dependency lockfile (`cargo update`).
- OpenAI: raw Responses SSE event processor now exits immediately when the downstream receiver is dropped, avoiding wasted stream parsing/work after cancellation.
- Performance: replace implicit `String` cloning via `to_string()` with direct `clone()` in stream collection, secret command building, and object tool-call fallback paths to trim hot-path overhead.
- Performance: tighten lock/permit lifetimes across gateway auth/admin/proxy hot paths (A2A, MCP, OpenAI models, LiteLLM key flows, backend health/state handling, OpenAI-compatible context resolution) and replace eager fallback construction with lazy `*_or_else` variants to reduce avoidable contention/allocation overhead.
- Build: depend on `safe-fs-tools` via the external repo checkout (`../safe-fs-tools`) instead of a vendored copy.
- Gateway: cap responses-shim streaming `tool_calls[].index` fan-out to a fixed slot limit (DoS hardening against oversized indexes) and remove per-event fallback-id cloning in SSE translation to reduce hot-path allocations.
- Gateway: remove `router.default_backend` in favor of `router.default_backends` (weighted float `weight`).
- Gateway: refactor `ditto-gateway` CLI parsing into `src/bin/ditto_gateway/cli.rs` (usage now documents the `--addr` alias; adds parser tests).
- Gateway: refactor `ditto-gateway` binary composition to use real modules (`src/bin/ditto_gateway/mod.rs`) instead of `include!` (no behavior change; improves navigation/maintainability).
- Docs: align LiteLLM/AI SDK parity notes for observability (Prometheus already ships per-path/per-backend latency histograms) and document the Prometheus metrics contract.
- Gateway: redact sensitive fields in JSON logs, audit payloads, and devtools JSONL by default via `observability.redaction`; proxy OTel span paths now drop query strings.
- Docs: add “Superset Contract（兼容性口径）” page to define must/best-effort/non-goals and link it in the mdBook summary.
- Docs: make `TODO.md` trackable by adding an explicit unchecked backlog with DoD + verification commands.
- Profile: accept `max_context`/`max_context_window` and `best_context`/`best_context_window` config aliases.
- Profile: add prompt-cache capability/config flags (`ProviderCapabilities.prompt_cache` and `ModelConfig.prompt_cache`).
- Profile: replace `include!(".../partNN.rs")` with real modules (`auth`/`config`/`env`/`http`/`openai_*`).
- Utils: truncate non-2xx error bodies (default 64KiB) to avoid OOM in error paths.
- SDK: `stream_text` now uses bounded fan-out and only forwards to enabled streams (prevents unbounded buffering when one stream is not consumed).
- SDK: `stream_object` now uses bounded fan-out and only forwards to enabled streams (prevents unbounded buffering when one stream is not consumed).
- SDK: `stream_text` now avoids cloning `TextDelta` payloads when only the text stream is enabled (reduces per-chunk allocations on text-only consumers).
- SDK: `stream_object` now consumes stream chunks by value and releases internal state locks earlier in the hot path (reduces lock contention and redundant cloning under streaming load).
- SDK: `stream_object` now skips partial-JSON reparsing for non-content chunks (`Warnings`/`Usage`/`FinishReason`/etc.) and only reparses when text/tool buffers change (reduces streaming CPU overhead under event-heavy outputs).
- SDK: `CacheLayer` now stores cached stream chunks as `Arc<[StreamChunk]>`, replays hits without cloning an entire `Vec`, and releases cache mutexes earlier on hit paths (lower memory/lock overhead under concurrent reads).
- SDK: UI message stream SSE adapter now emits `start-step` / `finish-step` boundaries and synthesizes `tool-input-start` when tool deltas arrive before tool starts.
- SDK: add max-bytes caps for `StreamCollector` and `stream_object` internal buffers (emit warnings + truncate to reduce OOM risk on extremely large streams).
- Dev: pre-commit blocks `partNN.*` filenames in staged changes.
- Refactor: rename remaining `partNN.rs` files to descriptive module names.
- Dev: update the commit hook example scope to avoid legacy naming.
- API: remove `OpenAiProvider` in favor of `OpenAiModelsProvider` (breaking).
- Refactor: deduplicate stream aggregation via `stream::StreamCollector`.
- Refactor: add `utils::task::AbortOnDrop` for aborting background tasks on drop.
- OpenAI: parse raw Responses SSE via bounded SSE data parsing and truncate invalid event payloads in errors.
- Profile: reuse `utils::http::send_checked_json` for `/models` discovery errors (include non-2xx body).
- Gateway: proxy cache size caps now also apply when storing L2 responses into Redis.
- Gateway: Redis budget/cost reservation hashes no longer expire; stale reservations can be recovered via `POST /admin/reservations/reap` instead of silently wedging ledgers.
- Gateway: in-memory rate limiter now GC's per-minute usage to avoid unbounded key growth.
- Gateway: Anthropic/Google streaming adapters now parse SSE via bounded reader-based parsing (avoid deprecated line-based parsing).
- Gateway: passthrough proxy now avoids buffering large non-SSE responses (streams them) and only buffers when a response is small enough for usage accounting or proxy caching (reduces OOM risk on large downloads).
- Gateway: proxy non-stream handler now uses bounded pre-buffering to enable usage parsing and proxy caching even when the upstream response omits `content-length` (falls back to streaming when the cap is exceeded).
- Gateway: proxy non-stream pre-buffering no longer trusts `content-length` and falls back to streaming when the actual body exceeds the cap.
- Gateway: bounded proxy pre-buffering now accumulates into a single buffer (avoids per-chunk allocations on chunked responses; reduces memory/CPU overhead).
- Gateway: reduce proxy hot-path allocations by parsing `usage` from typed/slice-based JSON (including SSE usage chunks) instead of building intermediate `serde_json::Value` trees and temporary `Bytes`.
- Gateway: preallocate proxy buffering from `content-length` when available and normalize bounded-body max-byte handling, reducing reallocations and fixing edge-case cap inconsistency.
- Gateway: Redis proxy-cache purge-all now deletes keys in SCAN batches (avoids building a huge in-memory key list).
- Gateway: cap non-streaming `/v1/responses` shim buffering (chat/completions → responses) to avoid OOM on large upstream bodies.
- Gateway: prune in-memory cache/budget scopes when virtual keys are updated/removed (avoids unbounded growth when keys churn).
- Gateway: prune in-memory rate limiter scopes when virtual keys are updated/removed (avoids unbounded growth when keys churn).
- Gateway: append admin write actions (key upsert/delete, backend reset, proxy cache purge) into sqlite/redis audit logs when a store is enabled.
- Gateway: multipart parser now records per-part `content-type` (used by translation endpoints for file/audio uploads).
- Gateway: abort background health-check task on shutdown to avoid leaking tasks in-process.
- Gateway: normalize OpenAI-compatible proxy paths to `/v1/*` equivalents for internal checks/metrics (e.g. `/chat/completions` → `/v1/chat/completions`).
- Gateway: `/key/info` no longer includes `token` inside the `info` object (matches LiteLLM behavior).
- Gateway: `/key/info` now falls back to the `Authorization` bearer token when `?key` is omitted (matches LiteLLM behavior).
- Gateway: routing rules now support optional exact matching (`rules[].exact=true`) which takes precedence over prefix rules, plus an optional `*` suffix for LiteLLM-style prefix patterns (e.g. `anthropic/*`).
- Gateway: config env expansion now supports LiteLLM-style `os.environ/ENV_KEY` strings (in addition to `${ENV_KEY}` placeholders).
- Bedrock: bound eventstream decoder message/buffer bytes to avoid OOM on malformed streams.
- Docs: remove legacy external repo references.
- Docs: expand the gap analysis and streaming docs with additional enterprise/DX gaps and memory-safety notes.
- Gateway: enrich proxy response logs with `provider`/`upstream_model` and detailed `usage` token breakdown (input/cache/output/reasoning/total).
- Gateway: allow Anthropic/Google shims to forward provider-style API keys (e.g. `x-api-key`, `x-goog-api-key`, `?key=`) as upstream `Authorization: Bearer ...` when virtual keys are disabled.
- Gateway: aggregate `/models` and `/v1/models` across all configured proxy backends (dedup by model id).
- Gateway: extract proxy-cache hit handling and proxy failure finalization helpers to keep the OpenAI-compatible passthrough handler under the pre-commit 1000-line cap (no behavior change).
- Gateway translation: switch lazy singleton client caches (files/batches/moderations/images) to `tokio::sync::OnceCell` to avoid duplicate concurrent initialization and reduce lock contention.
- CI: add a GitHub Actions workflow to run Rust gates (`fmt`/`clippy`/`test`) plus a small feature matrix (including `--no-default-features`).
- CI: extend the `--no-default-features` clippy matrix to cover provider-only builds for each provider feature (`openai`, `openai-compatible`, `anthropic`, `google`, `cohere`, `bedrock`, `vertex`) across all targets.
- CI: run JS/TS typecheck + build (pnpm workspaces) for `packages/*` and `apps/admin-ui`.
- CI: verify `llms.txt` is up to date (`ditto-llms-txt --check`).
- Build: add `pnpm-lock.yaml` and use frozen installs in CI for deterministic JS/TS builds.
- Dev: strengthen `githooks/pre-commit` with `cargo clippy -D warnings`, gateway regression checks, and a staged-diff guard that blocks newly introduced `let _ = ...await` in gateway runtime paths (unless explicitly annotated).

### Fixed

- SDK: deduplicate `stream::StreamCollector` warnings for empty `tool_call.id` chunks so malformed streams cannot grow warning buffers via repeated identical entries.
- Gateway: preserve MCP tool-call execution order during auto-exec loops for both chat-completions and responses flows, avoiding reordering in side-effectful tool chains.
- Gateway: fix proxy health-check task compilation on newer Rust toolchains by removing a non-generalized async-closure lifetime in backend iteration.
- Build: restore `cargo +1.92.0 {clippy,test} --all-features` compatibility by tightening `sdk::cache` option handling and fixing a missing `Message` import in gateway-translation tests.
- Security: ensure secret-command subprocesses are terminated on task cancellation (`kill_on_drop(true)`) and harden the Linux cancellation regression test without force-killing arbitrary PIDs.
- Tests: skip truncated HTTP-body localhost-bind regression in sandboxed environments that forbid local listen sockets, preventing environment-induced false negatives.
- Gateway: reserve `/v1/gateway` control-plane token budgets at request-prepare time and refund on backend failure, preventing concurrent in-flight requests from overspending shared budgets.
- Gateway: when proxy cache is enabled, only pre-buffer/cache non-stream responses with a known `content-length` within cap; unknown-length bodies now stay on the streaming passthrough path instead of returning `502` on cache buffering overflow.
- Gateway: proxy stream abort metrics finalization now falls back to async execution when the abort-finalizer queue is full (prevents dropped aborted-stream metrics).
- Gateway: log sqlite/redis token-ledger persistence failures in `/v1/gateway` instead of silently ignoring them.
- Gateway: remove panic-prone `expect/unwrap` from the HTTP proxy non-stream path and header construction (no behavior change).
- Gateway: reject invalid JSON request bodies early when `Content-Type: application/json` (returns `invalid_json`) to prevent guardrails bypass.
- Gateway: fix streaming multipart passthrough (/v1/files and /v1/audio/* uploads) spending so in-memory budgets are decremented even when store features are disabled.
- Gateway: fix `cargo check --all-features` for `gateway-translation` after proxy attempt param cleanup (no behavior change).
- Gateway: fix clippy lint in LiteLLM key regeneration handler (no behavior change).
- Gateway: include translation-backed models in `GET /v1/models` when `gateway-translation` backends are configured (returns `200` even when no proxy backends are configured).
- Build: fix `cargo clippy --no-default-features -- -D warnings` by tightening cfg gating for provider/gateway-only HTTP helpers.
- Docs: fix parity notes to reflect MCP tools integration support for `POST /v1/responses`.
- Docs: fix MCP gateway docs drift for `/v1/responses` and refresh `llms.txt`.
- Docs: make `ditto-llms-txt` accept case-insensitive markdown links from `docs/src/SUMMARY.md` and refresh generated `llms.txt` outputs.
- Docs: update admin UI setup instructions to use pnpm (workspaces).
- Gateway: when cost budgets (`total_usd_micros`) are enabled, treat non-token endpoints as unsupported (`cost_budget_unsupported_endpoint`) to avoid mis-accounting or bypass.
- Security: add timeouts and a 64KiB output cap when resolving `secret://...` via external CLIs (configurable via `DITTO_SECRET_COMMAND_TIMEOUT_MS/SECS`).
- Security: harden `ProviderAuth::*_command` by adding timeouts and a 64KiB output cap (configurable via `DITTO_AUTH_COMMAND_TIMEOUT_MS/SECS`); command stdout may now be plain text, a JSON string, or a JSON object (`api_key`/`token`/`access_token`).
- Security: on auth/secret command timeout, explicitly abort and join stdout/stderr reader tasks before returning (prevents detached task buildup under repeated timeouts).
- Gateway: harden admin auth by rejecting `/admin/*` when admin tokens are not configured (returns `not_configured`; avoids default-allow).
- Gateway: strip `proxy-authorization`/`x-forwarded-authorization` and hop-by-hop headers when proxying requests upstream.
- Gateway: avoid leaking raw virtual keys/prompts and reduce allocations by making `GatewayRequest::cache_key` return a hex digest, and using hashed route seeds for weighted backend selection.
- Security: redact `GatewayRequest` debug output to avoid logging virtual keys and prompts (keeps only lengths + hashes).
- Gateway: avoid persisting deleted raw key tokens in `/key/delete` audit payload and redact audit reads/exports to reduce token leakage risk.
- Gateway: settle passthrough SSE stream budgets using the final `usage` chunk when present (prefer actual usage over request estimates).
- Gateway: stream large multipart uploads to upstream for `/v1/files` and `/v1/audio/{transcriptions,translations}` (avoid buffering the full request body).
- Gateway translation: return `501 unsupported_feature` for backends requiring a build-time-disabled capability (distinguish from upstream failures).
- Gateway: fix Google GenAI streaming encoding to emit incremental text deltas (avoids duplicated output when clients concatenate chunks).
- Gateway: bound MCP backend JSON-RPC response bodies and truncate error body snippets (avoid OOM/huge error logs on oversized responses).
- Gateway: MCP `tools/list` now follows `nextCursor` (up to 8 pages) to return a complete tool list and returns `nextCursor` when a `cursor` is explicitly provided (single-server only).
- Gateway: harden `/mcp/tools/list` and `/mcp/tools/call` error handling (returns `invalid_json`/`request_too_large`, maps `invalid_request` vs backend failures, and returns JSON errors for missing/unknown MCP servers) and avoid cursor-driven cache growth for `tools/list` (cap cursor bytes; bypass cache when cursor is set).
- Gateway: make `/mcp` (JSON-RPC) errors consistently return JSON-RPC error objects for parse/invalid requests and MCP server selection failures, and return `413` on oversized requests.
- Gateway: cap per-backend `/v1/models` aggregation response size (skips oversized backends).
- SDK: cap OpenAI-like binary responses (`/files/*/content`, `/audio/speech`) via bounded reads and expose `with_max_binary_response_bytes` for tuning (avoid OOM on large downloads).
- Gateway: proxy-cache buffering now uses bounded reads and returns `502 invalid_backend_response` on oversized bodies instead of silently truncating to empty (avoid OOM/incorrect responses).
- Build: decouple `openai-compatible` from `openai` and tighten cfg gating for OpenAI-like provider helpers (fixes `--no-default-features --features openai-compatible` builds).
- Build: vendor `safe-fs-tools` to remove the git dependency (agent feature can build offline).
- Providers: remove panic-prone `expect` from OpenAI-like images provider metadata assembly (no behavior change).
- Build: fix `--no-default-features --features <provider>` builds across providers by tightening cfg gating around streaming-only helpers and provider-scoped utilities (clippy `-D warnings`).
- Build: when `streaming` is disabled, OpenAI streaming entrypoints now return a clear error instead of failing to compile.
- Build: add `required-features` to examples so `cargo {check,clippy} --all-targets` works across provider-only feature sets.
- Gateway: refactor `ResponseCache::insert` to accept `CacheConfig` instead of a long argument list (no behavior change).

## [0.1.2] - 2026-02-01

### Added

- Unified SDK: `LanguageModel` / `EmbeddingModel` + core request/response types.
- Model middleware: `LanguageModelLayer` + `LayeredLanguageModel` + `LanguageModelLayerExt` for composable wrappers.
- Generate request: add optional OpenAI-style fields (`seed`, `presence_penalty`, `frequency_penalty`, `logprobs`, `top_logprobs`, `user`) with OpenAI-compatible mapping and warnings when unsupported.
- AI SDK-aligned helpers: `generate_text` / `stream_text`, `generate_object_json` / `stream_object` (structured outputs), and `embed_many` aliases.
- Structured output options: `ObjectOptions` (`output=Object|Array`, `strategy=Auto|NativeSchema|ToolCall|TextJson`) and streaming `element_stream` for array outputs.
- Multi-modal message parts: `ContentPart::Image` (images) and `ContentPart::File` (PDFs) with `FileSource` support.
- Providers: OpenAI (Responses + embeddings), Anthropic (Messages), Google (GenAI + embeddings).
- Provider: OpenAI-compatible Chat Completions (for LiteLLM / DeepSeek / Qwen / etc.) and embeddings.
- Provider: Cohere Chat API (`/v2/chat`) with generate + SSE streaming + tool calls.
- Gateway translation: allow `provider=cohere` backends for OpenAI-compatible translation endpoints.
- Gateway translation: accept LiteLLM-style OpenAI-compatible provider aliases (e.g. `groq`, `mistral`, `deepseek`, `openrouter`).
- Gateway translation: support `POST /v1/responses/compact` via provider-backed compaction (best-effort).
- Streaming + tool calling support across providers (with compatibility warnings when unsupported).
- Stream utility: `collect_stream(StreamResult) -> CollectedStream` to aggregate `StreamChunk`s into a final `GenerateResponse`.
- Streaming: `abortable_stream(StreamResult) -> AbortableStream` with `StreamAbortHandle`.
- Provider builders accept a custom `reqwest::Client` via `with_http_client` (proxy/headers/timeout customization).
- Provider config: `ProviderConfig.http_headers` to apply default HTTP headers when building clients from config (also used for `/models` discovery).
- Provider config: `ProviderConfig.http_query_params` to apply default HTTP query params when building clients from config (also used for `/models` discovery).
- File upload helper for OpenAI and OpenAI-compatible providers: `upload_file` / `upload_file_with_purpose`.
- Examples: `basic`, `streaming`, `tool_calling`, `embeddings`, `openai_compatible`, `openai_compatible_embeddings`, `multimodal`.
- Roadmap: `TODO.md` with a scoped capability checklist (LiteLLM / AI SDK aligned).
- Optional integration smoke tests behind the `integration` feature (requires real API keys).
- Utilities: generic SSE parsing and JSON Schema → OpenAPI schema conversion (for tool schemas).
- Tool schemas: document the supported JSON Schema subset contract and add regression coverage.
- Provider clients can be built from config: `*::from_config(&ProviderConfig, &Env)`.
- Auth helper: `resolve_auth_token_with_default_keys` (for provider-specific default env keys).
- Provider auth: `ProviderAuth::HttpHeaderEnv` / `ProviderAuth::HttpHeaderCommand` for non-standard auth headers (e.g. `api-key` gateways).
- Provider auth: `ProviderAuth::QueryParamEnv` / `ProviderAuth::QueryParamCommand` for gateways that require auth in the query string.
- Streaming emits request conversion warnings via `StreamChunk::Warnings`.
- Controlled request options via `ProviderOptions` (`reasoning_effort`, `response_format`).
- OpenAI Responses (raw): support `reasoning.summary` and parse `response.reasoning_text.delta` / `response.reasoning_summary_text.delta`.
- Streaming emits response ids (when available) via `StreamChunk::ResponseId`.
- OpenAI-only options via `ProviderOptions`: `parallel_tool_calls`.
- OpenAI Responses tool schemas default to `strict=true` when omitted.
- Usage: add `cache_input_tokens` (e.g., OpenAI `cached_tokens`) for prompt-cache accounting.
- Usage: add `cache_creation_input_tokens` (Anthropic / LiteLLM) for prompt-cache accounting.
- Gateway: pricing table supports LiteLLM prompt-cache costs (`cache_read_input_token_cost`, `cache_creation_input_token_cost`).
- Gateway: pricing table supports LiteLLM tiered costs (`*_above_*_tokens` keys).
- Gateway: pricing table supports LiteLLM service tier costs (`*_priority`, `*_flex`) and uses request `service_tier` for USD budget estimates.
- Gateway: cost budgeting accounts for per-backend `model_map` when pricing entries exist for the mapped model.
- Image generation: `ImageGenerationModel` + OpenAI/OpenAI-compatible `/images/generations`.
- Audio: `AudioTranscriptionModel` + `AudioTranslationModel` + `SpeechModel` for OpenAI/OpenAI-compatible `/audio/*`.
- Moderations: `ModerationModel` for OpenAI/OpenAI-compatible `/moderations`.
- Rerank: `RerankModel` + Cohere `/rerank`.
- Batches: `BatchClient` + OpenAI/OpenAI-compatible `/batches`.
- Document non-goals and optional future scope (gateway/control-plane features, agent loop, UI SDK surface, native auth adapters).
- Agent tool loop: `ToolLoopAgent` + `ToolExecutor`, stop hooks, approvals, and tool-result backfill (feature `agent`).
- Agent: built-in tool wrappers and executors for `ToolLoopAgent` (feature `agent`) backed by `safe-fs-tools`: `http_fetch`, `fs_read_file`, `fs_write_file`, `fs_list_dir`, `fs_find`, `fs_grep`, `fs_stat`, `fs_mkdir`, `fs_move`, `fs_copy_file`, `fs_delete_file`, `shell_exec`.
- Agent: add `safe-fs-tools` executors for `fs_list_dir`, `fs_stat`, `fs_mkdir`, `fs_move`, `fs_copy_file`.
- Agent: `shell_exec` supports optional `stdin` (UTF-8) input.
- Agent: `http_fetch` supports `parse_json`, per-call `max_response_bytes`, emits `elapsed_ms`, and marks non-2xx responses as tool errors.
- Auth adapters: SigV4 signer + OAuth client-credentials flow (feature `auth`).
- Providers: Bedrock (SigV4) and Vertex (OAuth) minimal adapters (features `bedrock`, `vertex`).
- SDK utilities: stream protocol v1, telemetry sink, devtools JSONL logger, MCP tool adapter (feature `sdk`).
- SDK HTTP helpers: encode stream protocol v1 as NDJSON or SSE (feature `sdk`).
- Gateway control-plane: virtual keys, limits, cache, budget, routing, guardrails, passthrough, and `ditto-gateway` stub binary (feature `gateway`).
- Docs: clarify Bedrock/Vertex scope for minimal adapters.
- Bedrock: Anthropic Messages-on-Bedrock generate + streaming + tools support (feature `bedrock`).
- Vertex: GenAI generateContent + streamGenerateContent (SSE) + tools support (feature `vertex`).
- Gateway: `ditto-gateway` HTTP server with `/v1/gateway`, `/health`, `/metrics`, and `/admin/keys` (feature `gateway`).
- Gateway: OpenAI-compatible passthrough proxy for `ANY /v1/*` (incl. streaming) with per-backend header/query-param injection and optional devtools JSONL logging (feature `gateway-devtools`).
- Gateway: accept virtual keys via `x-api-key` (alias for `Authorization: Bearer ...` / `x-ditto-virtual-key`).
- Gateway: config supports `${ENV_VAR}` interpolation in proxy backend `base_url`/`headers`/`query_params`, backend `provider_config` fields, and `virtual_keys[].token` (resolved at startup).
- Gateway: `ditto-gateway` supports `--dotenv PATH` to load env vars for config interpolation and provider auth.
- Gateway: `ditto-gateway` supports `--admin-token-env` and `--redis-env` to load sensitive CLI options from env (works with `--dotenv`).
- Gateway: passthrough proxy supports per-backend model mapping via `model_map` (rewrites JSON `model` before forwarding).
- Gateway: add `gateway-translation` feature to serve `POST /v1/chat/completions` and `POST /v1/responses` via native Ditto providers (configured via backend `provider` + `provider_config`).
- Gateway: translation backends can also serve `POST /v1/embeddings` (best-effort OpenAI shape).
- Gateway: translation backends can also serve `POST /v1/moderations` and `POST /v1/images/generations` (best-effort OpenAI shapes).
- Gateway: translation backends can also serve `POST /v1/audio/transcriptions` and `POST /v1/audio/speech` (best-effort OpenAI behavior).
- Gateway: translation backends can also serve `POST /v1/rerank` (best-effort OpenAI behavior).
- Gateway: translation backends can also serve `GET|POST /v1/batches`, `GET /v1/batches/{id}`, and `POST /v1/batches/{id}/cancel` (best-effort OpenAI behavior).
- Gateway: guardrails support model allow/deny lists (exact or `prefix*`).
- Gateway: router rules support per-route guardrails overrides (by `model_prefix`).
- Gateway: when upstream does not implement `POST /v1/responses` (404/405/501), automatically fall back to `POST /v1/chat/completions` and return a best-effort Responses-like response/stream (`x-ditto-shim: responses_via_chat_completions`).
- Gateway: `--state PATH` persists admin virtual-key mutations to a JSON state file; proxy responses include `x-ditto-request-id`.
- Gateway: router supports weighted backends (`default_backends` / `rules[].backends`) and falls back on network errors when proxying.
- Gateway: optional sqlite persistence for admin virtual keys via `--sqlite PATH` (feature `gateway-store-sqlite`).
- Gateway: optional in-memory proxy cache for non-streaming OpenAI-compatible responses (feature `gateway-proxy-cache`).
- Gateway: proxy cache supports Redis-backed sharing when running with `--redis` (feature `gateway-store-redis`).
- Gateway: proxy cache supports admin purge and emits `x-ditto-cache-key` / `x-ditto-cache-source` headers (feature `gateway-proxy-cache`).
- Gateway: optional OpenTelemetry tracing exporter via OTLP (feature `gateway-otel`).
- Gateway: Prometheus metrics for per-backend in-flight gauge and request duration histogram (feature `gateway-metrics-prometheus`).
- Gateway: Prometheus metrics for per-path request counts and proxy request duration histogram (feature `gateway-metrics-prometheus`).
- Gateway: Prometheus metrics for proxy cache lookups/hits/misses (by path/source) and cache store/purge counters (feature `gateway-metrics-prometheus` + `gateway-proxy-cache`).
- Gateway: proxy backpressure via `--proxy-max-in-flight` (rejects when too many in-flight proxy requests).
- Gateway: per-backend proxy backpressure via `backends[].max_in_flight` (rejects with 429 + OpenAI-style error code `inflight_limit_backend`).
- Gateway: per-backend proxy request timeout via `backends[].timeout_seconds` (default: 300s).
- Gateway: optional active health checks for proxy backends (`--proxy-health-check*`, feature `gateway-routing-advanced`).
- Gateway: best-effort usage-based settling for proxy budgets (for non-streaming JSON responses, prefer `usage` tokens/cost over request estimates).
- Gateway: optional tiktoken-based input token counting for proxy budgets/guardrails/costing (feature `gateway-tokenizer`; falls back to request-size estimation).
- Gateway: virtual keys support optional `project_id` and `user_id` attribution; admin endpoints can aggregate `/admin/budgets` and `/admin/costs` by project/user.
- Gateway: virtual keys support shared budgets scoped by `project_id` / `user_id` via `project_budget` / `user_budget` (token + USD micros).
- Gateway: guardrails support regex patterns (`banned_regexes`) and optional PII blocking (`block_pii`).
- Gateway: guardrails support optional request schema validation via `guardrails.validate_schema`.
- Gateway: add Claude Code / Anthropic Messages API compatibility (`POST /v1/messages`, `POST /v1/messages/count_tokens`) and Gemini-compatible generateContent endpoints (`POST /v1beta/models/*:generateContent`, `POST /v1beta/models/*:streamGenerateContent`, and `POST /v1internal:*GenerateContent`).
- Gateway: Google GenAI endpoints accept virtual keys via `x-goog-api-key` and `?key=` (same virtual key auth as OpenAI/Anthropic endpoints).
- Gateway: proxy backend `model_map` supports wildcard `*` to rewrite any requested model name (useful for model aliasing across clients).
- Docs: add a gateway recipe for driving Claude Code CLI and Gemini CLI via a localhost `ditto-gateway`.
- Gateway translation: support legacy `POST /v1/completions` (non-streaming + streaming).
- Gateway translation: serve `GET /v1/models` and `GET /v1/models/*` locally (no upstream OpenAI-compatible required).
- Gateway translation: support `POST /v1/audio/translations` (same parsing/response as transcriptions).
- SDK: add `AudioTranslationModel` (request/response aliases of transcription types).
- Gateway translation: support `POST /v1/files`, `GET /v1/files`, `GET|DELETE /v1/files/*`, and `GET /v1/files/*/content`.
- Gateway: add `--json-logs`, `--proxy-cache*`, and `--otel*` CLI flags to `ditto-gateway`.
- Gateway admin key listing redacts tokens by default; `?include_tokens=true` returns full tokens.
- Multimodal example requires `--features base64` to enable base64 encoding dependency.

### Changed

- Refactor crate layout into modules (`embedding`/`model`/`providers`/`types`/`utils`).
- Providers: deduplicate OpenAI/OpenAI-compatible images/audio/moderations adapters via shared OpenAI-like core helpers.
- Providers: reduce boilerplate in OpenAI/OpenAI-compatible images/audio/moderations wrappers (re-export modules + macro-based wrappers).
- Providers: deduplicate OpenAI/OpenAI-compatible batches adapters via a shared `openai_batches_common` implementation.
- Providers: deduplicate OpenAI/OpenAI-compatible embeddings adapters via a shared `openai_embeddings_common` implementation.
- Providers: reuse shared `providers::openai_like::OpenAiLikeClient` for OpenAI/OpenAI-compatible core HTTP/config/file helpers.
- Providers: refactor OpenAI-family text adapters to share request-body construction between `generate` and `stream`.
- Audio: allow forwarding `provider_options` on multipart transcription/translation requests (skipping reserved keys like `model`/`file`).
- SDK: split `sdk::http` implementation into include parts to stay well under the pre-commit file size limit.
- Gateway: split translation backend routing/settlement into smaller include parts to stay under the pre-commit file size limit.
- Gateway: extend request schema validation to cover multipart `/v1/audio/transcriptions`, `/v1/audio/translations`, and `POST /v1/files`.
- Extend `DittoError` with `Api` and `Io` variants for richer provider and streaming errors.
- `provider_options` supports per-provider buckets (`"*"` + provider ids) and passes through additional provider-specific keys where supported (conflicts are ignored with warnings).
- `provider_options`: accept `openai_compatible` as an alias bucket for `openai-compatible`, and add `bedrock`/`vertex` buckets.
- Format: rustfmt cleanup (no behavior changes).
- Format: rustfmt cleanup (imports order).
- Refactor gateway HTTP module: split `src/gateway/http.rs` into smaller include parts (core/proxy/admin/translation/proxy-backend) to reduce duplication and keep modules under the repo size limit.
- Refactor OpenAI-family providers: reuse shared `providers::openai_like` helpers across OpenAI and OpenAI-compatible adapters (embeddings/audio/images/moderations/batches).
- Refactor OpenAI-family providers: centralize endpoint URL joining via `providers::openai_like::join_endpoint` to reduce adapter duplication.
- Providers/Auth: centralize non-2xx HTTP status/body handling via `utils::http` helpers (`send_checked*`).
- Tests: split `tests/gateway_openai_proxy.rs` into parts and skip `httpmock`-based tests when the environment disallows binding `127.0.0.1` (sandbox compatibility).
- Tests: reuse `utils::test_support::should_skip_httpmock` for `httpmock`-based OAuth test (sandbox compatibility).
- Metrics: add `ditto_gateway_proxy_responses_by_path_status_total` counter for per-path response statuses (Prometheus).
- Metrics: enrich Prometheus proxy metrics with per-backend/per-model response status counters, rate-limited/guardrail/budget-exceeded counters (by key/model/path), and SSE streaming gauges/counters (connections/bytes/completed/errors/aborts).
- Gateway translation: respect `stream_options.include_usage` for chat completions streaming (only emit usage chunk when requested).
- Dev: pre-commit rejects oversized staged Rust files (default 1000 lines; configurable via `DITTO_MAX_RS_LINES`).
- Refactor: split `gateway::translation` module into sub-files (no behavior changes).
- Refactor: split `gateway::http` module into sub-files (no behavior changes).
- Refactor: split `gateway::interop` module into sub-files (no behavior changes).
- Refactor: split `gateway::redis_store` module into sub-files (no behavior changes).
- Refactor: split `providers::openai` module into sub-files (no behavior changes).
- Refactor: split `providers::openai_compatible` module into sub-files (no behavior changes).
- Refactor: split `providers::anthropic` module into sub-files (no behavior changes).
- Refactor: split `providers::bedrock` module into sub-files (no behavior changes).
- Refactor: split `providers::cohere` module into sub-files (no behavior changes).
- Refactor: split `providers::google` module into sub-files (no behavior changes).
- Refactor: split `agent::toolbox` module into sub-files (no behavior changes).
- Agent: switch `fs_delete_file` executor to `safe-fs-tools` `delete` API (unified delete).
- Docs: clarify `ignore_missing` behavior for `fs_delete_file` tool.
- Refactor: split `object` module into sub-files (no behavior changes).
- Refactor: split `profile` module into sub-files (no behavior changes).
- Dev: fix clippy warnings (`cargo clippy --all-targets --all-features -- -D warnings`).

### Changed

- Providers: centralize tool-call argument JSON parsing + warnings via `types::parse_tool_call_arguments_json_or_string`.
- Providers: reuse `utils::streaming::init_sse_stream` across SSE streaming adapters.
- Providers: centralize unsupported provider-options warnings via `types::warn_unsupported_provider_options`.
- Auth/Profile: remove legacy env-key aliases (use standard provider env keys or configure `ProviderAuth` explicitly).

### Fixed

- Docs: update README gateway translation endpoints list (`/v1/models`, `/v1/completions`, `/v1/audio/translations`).
- Docs: fix README `gateway.json` example indentation.
- Gateway: do not mount `/admin/*` routes unless an admin token is configured.
- Gateway: apply per-route guardrails overrides to OpenAI proxy requests.
- Gateway: extend request schema validation coverage (`/v1/completions`, `/v1/moderations`, `/v1/images/generations`, `/v1/audio/speech`, `/v1/rerank`, `/v1/batches`).
- Gateway tokenizer: estimate input tokens for additional OpenAI endpoints (`/v1/completions`, `/v1/images/generations`, `/v1/audio/speech`, `/v1/rerank`).
- Gateway translation: use `--dotenv` env values when lazily building provider clients (embeddings/moderations/images/audio/rerank/batches).
- Gateway proxy cache: include `x-api-key` in the cache scope when virtual keys are disabled.
- Gateway: keep proxy backpressure permits until the response body is drained (including non-streaming responses).
- Streaming: abort background stream tasks when the consumer streams are dropped.
- Providers: avoid panicking if the default `reqwest::Client` build fails (fall back to `reqwest::Client::new()`).
- Security: redact sensitive fields in `Debug` for gateway key config and auth-related types.
- Agent: `fs_write_file` rejects symlink traversal to prevent root escape side effects.
- Tests: make Vertex `generateContent` mock matching robust to float serialization differences.
- OpenAI Responses: map `finish_reason` consistently (generate + stream), including tool-call completion.
- OpenAI Responses: include `instructions` (from system messages) to satisfy providers that require it.
- OpenAI-compatible streaming: flush pending tool calls and always emit a final `FinishReason` even if the provider omits it.
- OpenAI-compatible: support legacy `function_call` (generate + stream) and map `finish_reason=\"function_call\"` to `ToolCalls`.
- OpenAI-compatible: map `ToolChoice::Required` to `tool_choice=\"required\"` (instead of silently degrading to `auto`).
- Parameter conversion: clamp out-of-range `temperature`/`top_p` and drop non-finite values with warnings (avoid silently sending `0`).
- Stop sequences: drop empty/duplicate entries; truncate to 4 for OpenAI-compatible + Anthropic with a warning (Google preserves count).
- Google tool schemas: resolve local JSON Schema `$ref`; emit `Warning::Compatibility(tool.parameters.$ref)` only for unresolvable refs.
- Google tool schemas: emit `Warning::Compatibility(tool.parameters.unsupported_keywords)` when tool parameter JSON Schema uses unsupported keywords (they are ignored).
- JSON Schema → OpenAPI conversion: support common constraints and `additionalProperties` for tool schemas.
- Tool call arguments: preserve raw JSON on parse failures, emit `Warning::Compatibility(tool_call.arguments)`, and avoid double-encoding when replaying assistant tool calls.
- `collect_stream`: preserve chunk ordering (text/reasoning/tool calls) and warn on invalid tool-call argument JSON.
- Audio transcriptions: fall back to text with a warning if JSON response parsing fails (avoid silently swallowing errors).
- Bedrock eventstream header parsing validates header value lengths for all types.

## [0.1.1] - 2026-01-23

### Added

- Provider profile config (`base_url` / auth / model whitelist / capability flags)
- OpenAI-compatible `GET /models` discovery
- Model-level `thinking` config (mapped by consumers to `reasoning.effort`)
